{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mass case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/workspace/data/VinDr-Mammo/'\n",
    "\n",
    "img_root = os.path.join(root, 'Mass', 'image')\n",
    "mask_root = os.path.join(root, 'Mass', 'mask')\n",
    "registered_img_root = os.path.join(root, 'Mass', 'registered_oppositeSide_image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = pd.read_csv(os.path.join(root, \"Mass\", \"trainset_normalized.csv\"))\n",
    "val_list = pd.read_csv(os.path.join(root, \"Mass\", \"valset_normalized.csv\"))\n",
    "test_list = pd.read_csv(os.path.join(root, \"Mass\", \"testset_normalized.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list.shape, val_list.shape, test_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = []\n",
    "valset = []\n",
    "testset = []\n",
    "\n",
    "for index, row in train_list.iterrows():\n",
    "    my_data = OrderedDict()\n",
    "\n",
    "    my_data['file_name'] = os.path.join(img_root, row['ID'])\n",
    "    my_data['mask_file_name'] = os.path.join(mask_root, row['ID'])\n",
    "    my_data['otherSide_file_name'] = os.path.join(registered_img_root, row['ID'])\n",
    "    my_data['additional_feature'] = list(row.values[1:])\n",
    "\n",
    "    # if not os.path.isfile(my_data['file_name']):\n",
    "    #     print(my_data['file_name'])\n",
    "    # if not os.path.isfile(my_data['mask_file_name']):\n",
    "    #     print(my_data['mask_file_name'])\n",
    "    # if not os.path.isfile(my_data['otherSide_file_name']):\n",
    "    #     print(my_data['otherSide_file_name'])\n",
    "\n",
    "    trainset.append(my_data)\n",
    "    \n",
    "for index, row in val_list.iterrows():\n",
    "    my_data = OrderedDict()\n",
    "\n",
    "    my_data['file_name'] = os.path.join(img_root, row['ID'])\n",
    "    my_data['mask_file_name'] = os.path.join(mask_root, row['ID'])\n",
    "    my_data['otherSide_file_name'] = os.path.join(registered_img_root, row['ID'])\n",
    "    my_data['additional_feature'] = list(row.values[1:])\n",
    "\n",
    "    # if not os.path.isfile(my_data['file_name']):\n",
    "    #     print(my_data['file_name'])\n",
    "    # if not os.path.isfile(my_data['mask_file_name']):\n",
    "    #     print(my_data['mask_file_name'])\n",
    "    # if not os.path.isfile(my_data['otherSide_file_name']):\n",
    "    #     print(my_data['otherSide_file_name'])\n",
    "\n",
    "    valset.append(my_data)\n",
    "    \n",
    "for index, row in test_list.iterrows():\n",
    "    my_data = OrderedDict()\n",
    "\n",
    "    my_data['file_name'] = os.path.join(img_root, row['ID'])\n",
    "    my_data['mask_file_name'] = os.path.join(mask_root, row['ID'])\n",
    "    my_data['otherSide_file_name'] = os.path.join(registered_img_root, row['ID'])\n",
    "    my_data['additional_feature'] = list(row.values[1:])\n",
    "\n",
    "    # if not os.path.isfile(my_data['file_name']):\n",
    "    #     print(my_data['file_name'])\n",
    "    # if not os.path.isfile(my_data['mask_file_name']):\n",
    "    #     print(my_data['mask_file_name'])\n",
    "    # if not os.path.isfile(my_data['otherSide_file_name']):\n",
    "    #     print(my_data['otherSide_file_name'])\n",
    "\n",
    "    testset.append(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_trainset_cnt = len(trainset)\n",
    "mass_valset_cnt = len(valset)\n",
    "mass_testset_cnt = len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trainset), len(valset), len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_root = \"/workspace/source\"\n",
    "# jsonl_file_path = os.path.join(save_root, \"VinDr-Mammo_onlyMass_train_metadata.jsonl\")\n",
    "# with open(jsonl_file_path, \"w\") as f:\n",
    "#     for metadata in trainset:\n",
    "#         json.dump(metadata, f)\n",
    "#         f.write(\"\\n\")\n",
    "\n",
    "# jsonl_file_path = os.path.join(save_root, \"VinDr-Mammo_onlyMass_validation_metadata.jsonl\")\n",
    "# with open(jsonl_file_path, \"w\") as f:\n",
    "#     for metadata in valset:\n",
    "#         json.dump(metadata, f)\n",
    "#         f.write(\"\\n\")\n",
    "\n",
    "# jsonl_file_path = os.path.join(save_root, \"VinDr-Mammo_onlyMass_test_metadata.jsonl\")\n",
    "# with open(jsonl_file_path, \"w\") as f:\n",
    "#     for metadata in testset:\n",
    "#         json.dump(metadata, f)\n",
    "#         f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_target = ['0f0551f4edb5494b0d8765c23fe421ae_R_MLO_bbox-1_BI-RADS-4_Mass.nii.gz', \n",
    "               '99e880e8d860ed86d42caa8728582335_R_MLO_bbox-1_BI-RADS-4_Mass.nii.gz', \n",
    "               'e1b3a40c60b61bc218029c3554db7467_R_MLO_bbox-1_BI-RADS-4_Mass.nii.gz', \n",
    "               '1caf7a7178dbc3fb9649b93ba42115a7_L_CC_bbox-1_BI-RADS-5_Skin-Retraction+Nipple-Retraction+Mass.nii.gz', \n",
    "               '99f8ee0074c65cc7c5b115e182cc9504_R_CC_bbox-1_BI-RADS-4_Mass.nii.gz'\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotset = []\n",
    "\n",
    "for id in plot_target:\n",
    "    my_data = OrderedDict()\n",
    "\n",
    "    my_data['file_name'] = os.path.join(img_root, id)\n",
    "    my_data['mask_file_name'] = os.path.join(mask_root, id)\n",
    "    my_data['otherSide_file_name'] = os.path.join(registered_img_root, id)\n",
    "    \n",
    "    # if not os.path.isfile(my_data['file_name']):\n",
    "    #     print(my_data['file_name'])\n",
    "    # if not os.path.isfile(my_data['mask_file_name']):\n",
    "    #     print(my_data['mask_file_name'])\n",
    "    # if not os.path.isfile(my_data['otherSide_file_name']):\n",
    "    #     print(my_data['otherSide_file_name'])\n",
    "    \n",
    "    \n",
    "    if id in train_list['ID'].values:\n",
    "        print(\"train\")\n",
    "        value = list(train_list[train_list['ID'] == id].values[0,1:])\n",
    "    elif id in val_list['ID'].values:\n",
    "        print(\"val\")\n",
    "        value = list(val_list[val_list['ID'] == id].values[0,1:])\n",
    "    elif id in test_list['ID'].values:\n",
    "        print(\"test\")\n",
    "        value = list(test_list[test_list['ID'] == id].values[0,1:])\n",
    "    else:\n",
    "        print(\"error\")\n",
    "\n",
    "    my_data['additional_feature'] = value\n",
    "    plotset.append(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_root = \"/workspace/source\"\n",
    "jsonl_file_path = os.path.join(save_root, \"VinDr-Mammo_plotVal_metadata.jsonl\")\n",
    "with open(jsonl_file_path, \"w\") as f:\n",
    "    for metadata in plotset:\n",
    "        json.dump(metadata, f)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findings = pd.read_csv(os.path.join(root, 'finding_annotations.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = os.path.join(root, 'Mass', 'Mass_dataset_split.json')\n",
    "with open(json_path, 'r') as json_file:\n",
    "    Mass_dataset = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = os.path.join(data_root, 'Normal', 'Normal_dataset_split.json')\n",
    "with open(json_path, 'r') as json_file:\n",
    "    Normal_dataset = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_root = os.path.join(root, 'Normal', 'image')\n",
    "registered_img_root = os.path.join(root, 'Normal', 'registered_oppositeSide_image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = os.listdir(img_root)\n",
    "registered_list = os.listdir(registered_img_root)\n",
    "len(img_list), len(registered_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainset = []\n",
    "# valset = []\n",
    "# testset = []\n",
    "\n",
    "imgs = os.listdir(registered_img_root)\n",
    "\n",
    "for img in tqdm(imgs):\n",
    "    if \".nii.gz\" not in img:\n",
    "        continue\n",
    "    \n",
    "    img_name_split = img.split('.')[0].split('_')\n",
    "    pat_id = img_name_split[0]\n",
    "    side = img_name_split[1]\n",
    "    view = img_name_split[2]\n",
    "\n",
    "    img_path = os.path.join(img_root, img)\n",
    "    registered_img_path = os.path.join(registered_img_root, img)\n",
    "\n",
    "    target_row = findings[(findings['study_id'] == pat_id) & (findings['laterality'] == side) & (findings['view_position'] == view)]\n",
    "\n",
    "    density = target_row['breast_density'].values[0].split(\" \")[1]\n",
    "    if density == \"A\" or density == \"B\":\n",
    "        density = 0\n",
    "    else:\n",
    "        density = 1\n",
    "\n",
    "    birads = 0\n",
    "\n",
    "    my_data = OrderedDict()\n",
    "    my_data['file_name'] = img_path\n",
    "    my_data['mask_file_name'] = None\n",
    "    my_data['otherSide_file_name'] = registered_img_path\n",
    "    my_data['additional_feature'] = [0]*67+[density, birads]\n",
    "\n",
    "    # if not os.path.isfile(my_data['file_name']):\n",
    "    #     print(my_data['file_name'])\n",
    "    # if not os.path.isfile(my_data['otherSide_file_name']):\n",
    "    #     print(my_data['otherSide_file_name'])\n",
    "\n",
    "    if pat_id in Normal_dataset['trainset']:\n",
    "        trainset.append(my_data)\n",
    "    elif pat_id in Normal_dataset['valset']:\n",
    "        valset.append(my_data)\n",
    "    elif pat_id in Normal_dataset['testset']:\n",
    "        testset.append(my_data)\n",
    "    elif pat_id in Mass_dataset['trainset'] or pat_id in Mass_dataset['valset'] or pat_id in Mass_dataset['testset']:\n",
    "        # print(\"Mass dataset\", pat)\n",
    "        continue\n",
    "    else:\n",
    "        print(\"Error,,,\", pat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trainset), len(valset), len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_root = \"/workspace/source\"\n",
    "jsonl_file_path = os.path.join(save_root, \"VinDr-Mammo_train_metadata.jsonl\")\n",
    "with open(jsonl_file_path, \"w\") as f:\n",
    "    for metadata in trainset:\n",
    "        json.dump(metadata, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "jsonl_file_path = os.path.join(save_root, \"VinDr-Mammo_validation_metadata.jsonl\")\n",
    "with open(jsonl_file_path, \"w\") as f:\n",
    "    for metadata in valset:\n",
    "        json.dump(metadata, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "jsonl_file_path = os.path.join(save_root, \"VinDr-Mammo_test_metadata.jsonl\")\n",
    "with open(jsonl_file_path, \"w\") as f:\n",
    "    for metadata in testset:\n",
    "        json.dump(metadata, f)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_train_prompt = []\n",
    "for data in trainset[:mass_trainset_cnt]:\n",
    "    mass_train_prompt.append(data['additional_feature'])\n",
    "mass_train_prompts = pd.DataFrame(mass_train_prompt)\n",
    "\n",
    "normal_train_prompt = []\n",
    "for data in trainset[mass_trainset_cnt:]:\n",
    "    normal_train_prompt.append(data['additional_feature'])\n",
    "normal_train_prompts = pd.DataFrame(normal_train_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_val_prompt = []\n",
    "for data in valset[:mass_valset_cnt]:\n",
    "    mass_val_prompt.append(data['additional_feature'])\n",
    "mass_val_prompts = pd.DataFrame(mass_val_prompt)\n",
    "\n",
    "normal_val_prompt = []\n",
    "for data in valset[mass_valset_cnt:]:\n",
    "    normal_val_prompt.append(data['additional_feature'])\n",
    "normal_val_prompts = pd.DataFrame(normal_val_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_test_prompt = []\n",
    "for data in testset[:mass_testset_cnt]:\n",
    "    mass_test_prompt.append(data['additional_feature'])\n",
    "mass_test_prompts = pd.DataFrame(mass_test_prompt)\n",
    "\n",
    "normal_test_prompt = []\n",
    "for data in testset[mass_testset_cnt:]:\n",
    "    normal_test_prompt.append(data['additional_feature'])\n",
    "normal_test_prompts = pd.DataFrame(normal_test_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mass_train_prompts), len(normal_train_prompts), len(mass_test_prompts), len(normal_test_prompts), len(mass_val_prompts), len(normal_val_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_root = \"/workspace/source\"\n",
    "\n",
    "mass_train_prompts.to_csv(os.path.join(save_root, \"mass_train_prompts.csv\"), index=False)\n",
    "normal_train_prompts.to_csv(os.path.join(save_root, \"normal_train_prompts.csv\"), index=False)\n",
    "mass_test_prompts.to_csv(os.path.join(save_root, \"mass_test_prompts.csv\"), index=False)\n",
    "normal_test_prompts.to_csv(os.path.join(save_root, \"normal_test_prompts.csv\"), index=False)\n",
    "mass_val_prompts.to_csv(os.path.join(save_root, \"mass_val_prompts.csv\"), index=False)\n",
    "normal_val_prompts.to_csv(os.path.join(save_root, \"normal_val_prompts.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
